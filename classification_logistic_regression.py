# -*- coding: utf-8 -*-
"""classification_20230902.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1y4umEOgSieB3H2dkZHXPkh8LcWGAtZ7_
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import RobustScaler
from imblearn.over_sampling import SMOTE
from sklearn import metrics
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform
from joblib import dump, load

"""# Get the Data

## Join Clustered Data with Cleaned Data
"""

# df_clustered = pd.read_csv('./data/output_cluster_kmeansv2.csv', index_col=0)
# df = pd.read_csv('./data/clean_data_processed.csv', index_col=0)
# df['cluster'] = df_clustered['cluster'].values.ravel()
# df.to_csv('./mydata/clean_clustered_data_classification.csv')

"""## Process Clean Clustered Data"""

# df_1 = df.drop(
#     [
#         'variation_0_color', 'variation_1_color', 'variation_0_image',
#         'variation_1_image', 'image_url', 'url', 'image_variation',
#         'name', 'category', 'id', 'model'
#     ],
#     axis=1)

"""### Modify Brand-related columns
Check if an item has a brand  
Check if an item has an URL of its brand
"""

# df_1['has_brand'] = df_1['brand'].apply(lambda x: 1 if not pd.isnull(x) else 0)
# df_1['has_brand_url'] = df_1['brand_url'].apply(lambda x: 1 if not pd.isnull(x) else 0)

# df_1 = df_1.drop(
#     [
#         'brand', 'brand_url'
#     ], axis=1)

"""### Process Country information
Check if an item is sold in a particular country among the six countries (ID, MY, PH, SG, TH, VN)
"""

# df_1['codCountry'] = df_1['codCountry'].apply(lambda x: x if not pd.isnull(x) else '0')

# df_1['cod_ID'] = df_1['codCountry'].apply(lambda x: 1 if x.find('ID') != -1 else 0)
# df_1['cod_MY'] = df_1['codCountry'].apply(lambda x: 1 if x.find('MY') != -1 else 0)
# df_1['cod_PH'] = df_1['codCountry'].apply(lambda x: 1 if x.find('PH') != -1 else 0)
# df_1['cod_SG'] = df_1['codCountry'].apply(lambda x: 1 if x.find('SG') != -1 else 0)
# df_1['cod_TH'] = df_1['codCountry'].apply(lambda x: 1 if x.find('TH') != -1 else 0)
# df_1['cod_VN'] = df_1['codCountry'].apply(lambda x: 1 if x.find('VN') != -1 else 0)

# df_1 = df_1.drop(['codCountry'], axis=1)

"""### Process subcategory information
Rank subcategory from 0 to 4 based on its average likes count per item
"""

# subcate_number_of_items = (df_1['subcategory'].value_counts()).to_dict()
# subcate_likes_sum_dict = dict()

# def sum_likes_count_per_subcate(subcate_name, likes_count):
#     if subcate_name not in subcate_likes_sum_dict.keys():
#         subcate_likes_sum_dict[subcate_name] = 0
#     subcate_likes_sum_dict[subcate_name] += likes_count

# for x, y in zip(df_1['subcategory'], df_1['likes_count']):
#     sum_likes_count_per_subcate(x, y)

# likes_per_subcate_dict = dict()
# for k, v in subcate_likes_sum_dict.items():
#     likes_per_subcate_dict[k] = v // subcate_number_of_items[k]

# likes_per_subcate_rank_1 = list()
# likes_per_subcate_rank_2 = list()
# likes_per_subcate_rank_3 = list()
# likes_per_subcate_rank_4 = list()
# likes_per_subcate_rank_5 = list()

# for k, v in likes_per_subcate_dict.items():
#     if v < 50:
#         likes_per_subcate_rank_5.append(k)
#     elif 50 <= v < 100:
#         likes_per_subcate_rank_4.append(k)
#     elif 100 <= v < 150:
#         likes_per_subcate_rank_3.append(k)
#     elif 150 <= v < 250:
#         likes_per_subcate_rank_2.append(k)
#     else: # v >= 230
#         likes_per_subcate_rank_1.append(k)

# print(len(likes_per_subcate_rank_5))
# print(len(likes_per_subcate_rank_4))
# print(len(likes_per_subcate_rank_3))
# print(len(likes_per_subcate_rank_2))
# print(len(likes_per_subcate_rank_1))

# def process_likes_per_subcate(x):
#     if x in likes_per_subcate_rank_5:
#         return 0
#     elif x in likes_per_subcate_rank_4:
#         return 1
#     elif x in likes_per_subcate_rank_3:
#         return 2
#     elif x in likes_per_subcate_rank_2:
#         return 3
#     else:
#         return 4

# df_1['subcategory_rank'] = df_1['subcategory'].apply(process_likes_per_subcate)

# df_1 = df_1.drop(['subcategory'], axis=1)

"""### Process likes information
The clustering result reveals that data is clustered based on likes count.  
The classifier is aimed to be used on new, unseen data which does not have likes count information yet. Estimating the potential likes count could be seen as the objective of the classification task.

"""

# df_1 = df_1.drop(['likes_count'], axis=1)

"""### Feature Scaling
Using RobustScaler for better handling outliers.  
Features need scaling: `current_price`, `raw_price`, `discount`
"""

# df_toscale = df_1[['current_price', 'raw_price', 'discount']]
# df_processed = df_1.drop(
#     [
#         'current_price', 'raw_price', 'discount'
#     ],
#     axis=1
# )

# transformer = RobustScaler()
# scaled = transformer.fit_transform(df_toscale)
# df_scaled = pd.DataFrame(scaled, columns=['current_price', 'raw_price', 'discount'])

# df_processed['current_price'] = df_scaled['current_price'].values.ravel()
# df_processed['raw_price'] = df_scaled['raw_price'].values.ravel()
# df_processed['discount'] = df_scaled['discount'].values.ravel()

"""### Save Dataset"""

# df_processed.to_csv('./processed_clean_clustered_data_classification_20230901_11h32.csv')

"""## Load Data"""

df_processed = pd.read_csv("./processed_clean_clustered_data_classification_20230901_11h32.csv", index_col=0)

"""# Dataset Preparation"""

split_1 = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=84)
for train_index, test_valid_index in split_1.split(df_processed, df_processed["cluster"]):
    train_set = df_processed.iloc[train_index]
    test_valid_set = df_processed.iloc[test_valid_index]

split_2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=84)
for test_index, valid_index in split_2.split(test_valid_set, test_valid_set["cluster"]):
    test_set = test_valid_set.iloc[test_index]
    valid_set = test_valid_set.iloc[valid_index]

X_train = train_set.drop(["cluster"], axis=1)
y_train = train_set[["cluster"]].values.ravel()

X_valid = valid_set.drop(["cluster"], axis=1)
y_valid = valid_set[["cluster"]].values.ravel()

X_test = test_set.drop(["cluster"], axis=1)
y_test = test_set[["cluster"]].values.ravel()

"""### Apply SMOTE to the training data"""

smote = SMOTE(random_state=84)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Count the occurrences of each class in y_train_resampled
class_counts = np.bincount(y_train_resampled)
for class_label, count in enumerate(class_counts):
    print(f"Class {class_label}: {count}Â samples")

"""# Train Model"""

log_reg = LogisticRegression(multi_class="multinomial", penalty='elasticnet',
                             l1_ratio=0.2, max_iter=10000, solver='saga', C=10)

log_reg.fit(X_train_resampled[list(X_train_resampled.keys())], y_train_resampled)

y_valid_pred = log_reg.predict(X=X_valid[list(X_valid.keys())])

print(metrics.classification_report(y_valid, y_valid_pred))

print(metrics.multilabel_confusion_matrix(y_valid, y_valid_pred))

"""# Fine-Tune Model"""

distributions = dict(C=uniform(loc=0, scale=10),
                     penalty=['l2', 'l1', 'elasticnet'],
                     max_iter=np.arange(10000, 20000, 1000).tolist())

clf = RandomizedSearchCV(log_reg, distributions, cv=5,
                         return_train_score=True, random_state=84)

search = clf.fit(X_train, y_train)

print("Best Params:", search.best_params_)
print("Best Score:", search.best_score_)
print("Best Estimator:", search.best_estimator_)

tuned_log_reg = LogisticRegression(C=9.313499666400707, max_iter=10000,
                                   multi_class='multinomial', penalty='l1', solver='saga')

tuned_log_reg.fit(X_train_resampled[list(X_train_resampled.keys())], y_train_resampled)

y_valid_pred = tuned_log_reg.predict(X=X_valid[list(X_valid.keys())])

print(metrics.classification_report(y_valid, y_valid_pred))

print(metrics.multilabel_confusion_matrix(y_valid, y_valid_pred))

"""## Test on Test Set:"""

y_test_pred = tuned_log_reg.predict(X=X_test[list(X_test.keys())])

print(metrics.classification_report(y_test, y_test_pred))

print(metrics.multilabel_confusion_matrix(y_test, y_test_pred))

"""# Save Model:"""

dump(tuned_log_reg, './classification_logreg_20230902_1002.joblib')